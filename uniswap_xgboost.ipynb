{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    uni_ticker = \"UNI-USD\"\n",
    "    eth_ticker = \"ETH-USD\"\n",
    "    start = datetime.datetime(2019, 1, 1)\n",
    "    end = datetime.datetime(2024, 4, 1)\n",
    "    uni = yf.download(uni_ticker, start=start, end=end, interval=\"1d\")\n",
    "    eth = yf.download(eth_ticker, start=start, end=end, interval=\"1d\")\n",
    "    uni = uni.reset_index()\n",
    "    uni.to_csv(\"uni.csv\", index=False)\n",
    "    eth = eth.reset_index()\n",
    "    eth.to_csv(\"eth.csv\", index=False)\n",
    "    return uni, eth\n",
    "\n",
    "\n",
    "def process_data(uni: pd.DataFrame, eth: pd.DataFrame):\n",
    "    uni = uni[uni[\"Open\"] < 0.30]\n",
    "    uni = uni[[\"Date\", \"Open\"]]\n",
    "    eth = eth[[\"Date\", \"Open\"]]\n",
    "\n",
    "    uni.rename(columns={\"Open\": \"UNI\"}, inplace=True)\n",
    "    eth.rename(columns={\"Open\": \"ETH\"}, inplace=True)\n",
    "\n",
    "    df = pd.merge(uni, eth, on=\"Date\")\n",
    "    df.dropna(inplace=True)\n",
    "    df[\"price\"] = df[\"ETH\"] / df[\"UNI\"]\n",
    "    ret = 100 * (df[\"price\"].pct_change()[1:])\n",
    "    realized_vol = ret.rolling(5).std()\n",
    "    realized_vol = pd.DataFrame(realized_vol)\n",
    "    realized_vol.reset_index(drop=True, inplace=True)\n",
    "    returns_svm = ret**2  # returns squared\n",
    "    returns_svm = returns_svm.reset_index()\n",
    "    X = pd.concat([realized_vol, returns_svm], axis=1, ignore_index=True)\n",
    "    X = X[4:].copy()\n",
    "    X = X.reset_index()\n",
    "    X.drop(\"index\", axis=1, inplace=True)\n",
    "    X.drop(1, axis=1, inplace=True)\n",
    "    X.rename(columns={0: \"realized_vol\", 2: \"returns_squared\"}, inplace=True)\n",
    "    X[\"target\"] = X[\"realized_vol\"].shift(-5)\n",
    "    X.dropna(inplace=True)\n",
    "    Y = X[\"target\"]\n",
    "    X.drop(\"target\", axis=1, inplace=True)\n",
    "    n = 252\n",
    "    X_train = X.iloc[:-n]\n",
    "    X_test = X.iloc[-n:]\n",
    "    Y_train = Y.iloc[:-n]\n",
    "    Y_test = Y.iloc[-n:]\n",
    "    return X_train.to_numpy(), X_test.to_numpy(), Y_train.to_numpy(), Y_test.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-train (1359, 2)\n",
      "y-train (1359,)\n",
      "x-test (252, 2)\n",
      "y-test (252,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ng/0bmxnvj96057v3jysxjk_9xw0000gn/T/ipykernel_60339/653528502.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eth.rename(columns={\"Open\": \"ETH\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "uni, eth = download_data()\n",
    "X_train, X_test, Y_train, Y_test = process_data(uni, eth)\n",
    "print(\"x-train\", X_train.shape)\n",
    "print(\"y-train\", Y_train.shape)\n",
    "print(\"x-test\", X_test.shape)\n",
    "print(\"y-test\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and transpile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "n_estimators = 5  # Increase the number of trees\n",
    "max_depth = 10  # Increase the maximum depth of each tree\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "xgb_reg.fit(X_train, Y_train)\n",
    "\n",
    "# save model into json \n",
    "from giza.zkcook import serialize_model\n",
    "serialize_model(xgb_reg, \"uniswap_lp_xgboost.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 21.8232\n",
      "Mean Squared Error (MSE): 3058.2166\n",
      "Root Mean Squared Error (RMSE): 55.3011\n",
      "R-squared (R²): 0.0349\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # eval metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"R-squared (R²): {r2:.4f}\")\n",
    "\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "mae, mse, rmse, r2 = evaluate_model(xgb_reg, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:03\u001b[0m.\u001b[1;36m932\u001b[0m\u001b[1m]\u001b[0m No model id provided, checking if model exists ✅\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:03\u001b[0m.\u001b[1;36m934\u001b[0m\u001b[1m]\u001b[0m Model name is: uniswap_lp_xgboost\n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:04\u001b[0m.\u001b[1;36m184\u001b[0m\u001b[1m]\u001b[0m Model already exists, using existing model ✅ \n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:04\u001b[0m.\u001b[1;36m185\u001b[0m\u001b[1m]\u001b[0m Model found with id -> \u001b[1;36m862\u001b[0m! ✅\n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:04\u001b[0m.\u001b[1;36m811\u001b[0m\u001b[1m]\u001b[0m Version Created with id -> \u001b[1;36m2\u001b[0m! ✅\n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:04\u001b[0m.\u001b[1;36m812\u001b[0m\u001b[1m]\u001b[0m Sending model for transpilation ✅ \n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:25\u001b[0m.\u001b[1;36m658\u001b[0m\u001b[1m]\u001b[0m Transpilation is fully compatible. Version compiled and Sierra is saved at Giza ✅\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m Transpiling Model...\n",
      "\u001b[1A\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:26\u001b[0m.\u001b[1;36m294\u001b[0m\u001b[1m]\u001b[0m Downloading model ✅\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:37:26\u001b[0m.\u001b[1;36m301\u001b[0m\u001b[1m]\u001b[0m model saved at: uniswap_lp_xgboost\n",
      "\u001b[3m                  Model                   \u001b[0m\n",
      "┏━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mid \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mname              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mdescription\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n",
      "│ 862 │ uniswap_lp_xgboost │             │\n",
      "└─────┴────────────────────┴─────────────┘\n",
      "\u001b[3m                                    Version                                     \u001b[0m\n",
      "┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mversion\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1msize  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mstatus \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmessage\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mdescri…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcreate…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mlast_u…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mframew…\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n",
      "│ 2       │ 136764 │ COMPLE… │ Transp… │ Intial  │ 2024-0… │ 2024-0… │ CAIRO   │\n",
      "│         │        │         │ Succes… │ version │ 13:37:… │ 13:37:… │         │\n",
      "│         │        │         │ and     │         │         │         │         │\n",
      "│         │        │         │ Compil… │         │         │         │         │\n",
      "│         │        │         │ Succes… │         │         │         │         │\n",
      "└─────────┴────────┴─────────┴─────────┴─────────┴─────────┴─────────┴─────────┘\n"
     ]
    }
   ],
   "source": [
    "# transpile the model into Orion Cairo\n",
    "! giza transpile uniswap_lp_xgboost.json --output-path uniswap_lp_xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 862\n",
    "VERSION_ID = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K▰▱▱▱▱▱▱ Creating endpoint!t!\n",
      "\u001b[?25h\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:38:03\u001b[0m.\u001b[1;36m812\u001b[0m\u001b[1m]\u001b[0m Endpoint is successful ✅\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:38:03\u001b[0m.\u001b[1;36m817\u001b[0m\u001b[1m]\u001b[0m Endpoint created with id -> \u001b[1;36m410\u001b[0m ✅\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m16:38:03\u001b[0m.\u001b[1;36m818\u001b[0m\u001b[1m]\u001b[0m Endpoint created with endpoint URL: \u001b[4;94mhttps://endpoint-selimsheker-862-2-f14d7c7f-7i3yxzspbq-ew.a.run.app\u001b[0m 🎉\n"
     ]
    }
   ],
   "source": [
    "# create giza endpoint\n",
    "! giza endpoints deploy --model-id {MODEL_ID} --version-id {VERSION_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = 410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run verifiable inference using giza sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from giza.agents.model import GizaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(input, model_id, version_id):\n",
    "    model = GizaModel(id=model_id, version=version_id)\n",
    "\n",
    "    (result, proof_id) = model.predict(\n",
    "        input_feed={\"input\": input}, verifiable=True, model_category=\"XGB\"\n",
    "    )\n",
    "\n",
    "    return result, proof_id\n",
    "\n",
    "\n",
    "def execution():\n",
    "    input = X_test[1, :]\n",
    "\n",
    "    (result, proof_id) = prediction(input, MODEL_ID, VERSION_ID)\n",
    "\n",
    "    print(f\"Predicted value for input {input.flatten()[0]} is {result}\")\n",
    "\n",
    "    return result, proof_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting deserialization process...\n",
      "✅ Deserialization completed! 🎉\n",
      "Predicted value for input 22.38294835718384 is 16.21992\n",
      "Proof ID: 786477d48e784bf79c99c07c2c8f0ab2\n"
     ]
    }
   ],
   "source": [
    "result, proof_id = execution()\n",
    "print(f\"Proof ID: {proof_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROOF_ID = \"786477d48e784bf79c99c07c2c8f0ab2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m15:52:45\u001b[0m.\u001b[1;36m667\u001b[0m\u001b[1m]\u001b[0m Getting proof from endpoint \u001b[1;36m407\u001b[0m ✅ \n",
      "┏━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1mid  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mjob_id\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mproving_t…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcairo_exe…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmetrics  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mcreated_d…\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mrequest_…\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
      "│ 1271 │ 1463   │            │            │ {'provin… │ 2024-06-27 │           │\n",
      "│      │        │            │            │ 16.2721}  │ 12:50:47.… │           │\n",
      "└──────┴────────┴────────────┴────────────┴───────────┴────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# get proof\n",
    "! giza endpoints get-proof --endpoint-id {ENDPOINT_ID} --proof-id \"{PROOF_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m15:53:26\u001b[0m.\u001b[1;36m844\u001b[0m\u001b[1m]\u001b[0m Getting proof from endpoint \u001b[1;36m407\u001b[0m ✅ \n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m15:53:28\u001b[0m.\u001b[1;36m049\u001b[0m\u001b[1m]\u001b[0m Proof downloaded to uniswap_lp_xgboost_proof.proof ✅\n"
     ]
    }
   ],
   "source": [
    "# download proof\n",
    "! giza endpoints download-proof --endpoint-id {ENDPOINT_ID} --proof-id \"{PROOF_ID}\" --output-path uniswap_lp_xgboost_proof.proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m15:53:39\u001b[0m.\u001b[1;36m007\u001b[0m\u001b[1m]\u001b[0m Verifying proof\u001b[33m...\u001b[0m\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m15:53:40\u001b[0m.\u001b[1;36m425\u001b[0m\u001b[1m]\u001b[0m Verification result: \u001b[3;92mTrue\u001b[0m\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m27\u001b[0m \u001b[1;92m15:53:40\u001b[0m.\u001b[1;36m426\u001b[0m\u001b[1m]\u001b[0m Verification time: \u001b[1;36m0.458288615\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "!giza verify --proof-id 1271"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create giza agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! giza agents create --endpoint-id 401 --name uniswap_lp_agent --description uniswap_lp_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "from logging import getLogger\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from giza.agents import AgentResult, GizaAgent\n",
    "\n",
    "from giza_helpers.addresses import ADDRESSES\n",
    "from giza_helpers.lp_tools import get_tick_range\n",
    "from giza_helpers.uni_helpers import (approve_token, check_allowance, close_position,\n",
    "                         get_all_user_positions, get_mint_params)\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ[\"GIZA-AGENT-TEST-1_PASSPHRASE\"] = os.environ.get(\"GIZA-AGENT-TEST-1_PASSPHRASE\")\n",
    "sepolia_rpc_url = os.environ.get(\"SEPOLIA_RPC_URL\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(realized_vol: float, dec_price_change: float):\n",
    "    pct_change_sq = (100 * dec_price_change) ** 2\n",
    "    print(realized_vol, pct_change_sq)\n",
    "    X = np.array([realized_vol, pct_change_sq])\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # TODO: implement fetching onchain or from some other source\n",
    "    # hardcoding the values for now\n",
    "    realized_vol = 4.20\n",
    "    dec_price_change = 0.1\n",
    "    return realized_vol, dec_price_change\n",
    "\n",
    "\n",
    "def create_agent(\n",
    "    model_id: int, version_id: int, chain: str, contracts: dict, account: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a Giza agent for the volatility prediction model\n",
    "    \"\"\"\n",
    "    agent = GizaAgent(\n",
    "        contracts=contracts,\n",
    "        id=model_id,\n",
    "        version_id=version_id,\n",
    "        chain=chain,\n",
    "        account=account,\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "def predict(agent: GizaAgent, X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Predict the next day volatility.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input to the model.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted value.\n",
    "    \"\"\"\n",
    "    print(X)\n",
    "    prediction = agent.predict(input_feed={\"val\": X}, verifiable=True, job_size=\"XL\")\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_pred_val(prediction: AgentResult):\n",
    "    \"\"\"\n",
    "    Get the value from the prediction.\n",
    "\n",
    "    Args:\n",
    "        prediction (dict): Prediction from the model.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted value.\n",
    "    \"\"\"\n",
    "    # This will block the executon until the prediction has generated the proof\n",
    "    # and the proof has been verified\n",
    "    return prediction.value[0][0]\n",
    "\n",
    "\n",
    "def rebalance_lp(\n",
    "    tokenA_amount: int,\n",
    "    tokenB_amount: int,\n",
    "    pred_model_id: int,\n",
    "    pred_version_id: int,\n",
    "    account=\"dev\",\n",
    "    chain=f\"ethereum:sepolia:{sepolia_rpc_url}\",\n",
    "    nft_id=None,\n",
    "):\n",
    "    logger = getLogger(\"agent_logger\")\n",
    "    nft_manager_address = ADDRESSES[\"NonfungiblePositionManager\"][11155111]\n",
    "    tokenA_address = ADDRESSES[\"UNI\"][11155111]\n",
    "    tokenB_address = ADDRESSES[\"WETH\"][11155111]\n",
    "    pool_address = \"0x287B0e934ed0439E2a7b1d5F0FC25eA2c24b64f7\"\n",
    "    user_address = \"0xCBB090699E0664f0F6A4EFbC616f402233718152\"\n",
    "    pool_fee = 3000\n",
    "    logger.info(\"Fetching input data\")\n",
    "    realized_vol, dec_price_change = get_data()\n",
    "    logger.info(f\"Input data: {realized_vol}, {dec_price_change}\")\n",
    "    X = process_data(realized_vol, dec_price_change)\n",
    "    contracts = {\n",
    "        \"nft_manager\": nft_manager_address,\n",
    "        \"tokenA\": tokenA_address,\n",
    "        \"tokenB\": tokenB_address,\n",
    "        \"pool\": pool_address,\n",
    "    }\n",
    "    agent = create_agent(\n",
    "        model_id=pred_model_id,\n",
    "        version_id=pred_version_id,\n",
    "        chain=chain,\n",
    "        contracts=contracts,\n",
    "        account=account,\n",
    "    )\n",
    "    result = predict(agent, X)\n",
    "    predicted_value = get_pred_val(result)\n",
    "    logger.info(f\"Result: {result}\")\n",
    "    with agent.execute() as contracts:\n",
    "        logger.info(\"Executing contract\")\n",
    "        if nft_id is None:\n",
    "            positions = [\n",
    "                max(get_all_user_positions(contracts.nft_manager, user_address))\n",
    "            ]\n",
    "        else:\n",
    "            positions = [nft_id]\n",
    "        logger.info(f\"Closing the following positions {positions}\")\n",
    "        for nft_id in positions:\n",
    "            close_position(user_address, contracts.nft_manager, nft_id)\n",
    "        logger.info(\"Calculating mint params...\")\n",
    "        _, curr_tick, _, _, _, _, _ = contracts.pool.slot0()\n",
    "        if not check_allowance(\n",
    "            contracts.tokenA, nft_manager_address, account, tokenA_amount\n",
    "        ):\n",
    "            approve_token(contracts.tokenA, nft_manager_address, tokenA_amount)\n",
    "        if not check_allowance(\n",
    "            contracts.tokenB, nft_manager_address, account, tokenB_amount\n",
    "        ):\n",
    "            approve_token(contracts.tokenB, nft_manager_address, tokenB_amount)\n",
    "        tokenA_decimals = contracts.tokenA.decimals()\n",
    "        tokenB_decimals = contracts.tokenB.decimals()\n",
    "        predicted_value = predicted_value / 100 * 1.96  # convert to decimal %\n",
    "        lower_tick, upper_tick = get_tick_range(\n",
    "            curr_tick, predicted_value, tokenA_decimals, tokenB_decimals, pool_fee\n",
    "        )\n",
    "        mint_params = get_mint_params(\n",
    "            user_address,\n",
    "            contracts.tokenA.address,\n",
    "            contracts.tokenB.address,\n",
    "            tokenA_amount,\n",
    "            tokenB_amount,\n",
    "            pool_fee,\n",
    "            lower_tick,\n",
    "            upper_tick,\n",
    "        )\n",
    "        # step 5: mint new position\n",
    "        logger.info(\"Minting new position...\")\n",
    "        contract_result = contracts.nft_manager.mint(mint_params)\n",
    "        logger.info(\"SUCCESSFULLY MINTED A POSITION\")\n",
    "        logger.info(\"Contract executed\")\n",
    "\n",
    "    logger.info(f\"Contract result: {contract_result}\")\n",
    "    pprint.pprint(contract_result.__dict__)\n",
    "    logger.info(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenA_amount = 500\n",
    "tokenB_amount = 500\n",
    "\n",
    "rebalance_lp(tokenA_amount, tokenB_amount, MODEL_ID, VERSION_ID, account=\"giza-agent-test-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## xgboost transpile and prediction stats\n",
    "\n",
    "# - n_estimators = 5, max_depth = 10\n",
    "#     - transpile from json: ~1sec\n",
    "#     - transpile from onnx: ~1min\n",
    "#     - generate proof: ~5sec\n",
    "#     - verify proof: ~1sec\n",
    "\n",
    "# - n_estimators = 50, max_depth = 50\n",
    "#     - transpile: out of memory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
