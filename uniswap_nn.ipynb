{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        4.043332\n",
      "2      -52.461419\n",
      "3      -17.076879\n",
      "4       -6.215413\n",
      "5       61.033549\n",
      "          ...    \n",
      "1616    -0.105024\n",
      "1617    -1.990389\n",
      "1618     0.386418\n",
      "1619    -0.500657\n",
      "1620     0.337570\n",
      "Name: price, Length: 1620, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/selim/Desktop/firstbatch/giza/giza-notebooks/giza_helpers/prepare_data.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eth.rename(columns={\"Open\": \"ETH\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from giza_helpers.prepare_data import *\n",
    "uni, eth = download_data()\n",
    "X_train, X_test, Y_train, Y_test = process_data(uni, eth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>realized_vol</th>\n",
       "      <th>returns_squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41.234377</td>\n",
       "      <td>3725.094061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.524274</td>\n",
       "      <td>94.512324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.948475</td>\n",
       "      <td>168.159335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.666232</td>\n",
       "      <td>58.146380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.469835</td>\n",
       "      <td>201.413137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1354</th>\n",
       "      <td>1.629951</td>\n",
       "      <td>12.127320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1355</th>\n",
       "      <td>1.616969</td>\n",
       "      <td>0.205096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>1.686993</td>\n",
       "      <td>0.148979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>1.552290</td>\n",
       "      <td>0.003271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1358</th>\n",
       "      <td>1.618553</td>\n",
       "      <td>0.072228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1359 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      realized_vol  returns_squared\n",
       "0        41.234377      3725.094061\n",
       "1        41.524274        94.512324\n",
       "2        29.948475       168.159335\n",
       "3        25.666232        58.146380\n",
       "4        22.469835       201.413137\n",
       "...            ...              ...\n",
       "1354      1.629951        12.127320\n",
       "1355      1.616969         0.205096\n",
       "1356      1.686993         0.148979\n",
       "1357      1.552290         0.003271\n",
       "1358      1.618553         0.072228\n",
       "\n",
       "[1359 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import time\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    X_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    Y_train: pd.DataFrame,\n",
    "    Y_test: pd.DataFrame,\n",
    "):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(X_train.shape[1], 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1),\n",
    "    )\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.RMSprop(model.parameters())\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(Y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "    # Training loop\n",
    "    epochs_trial = np.arange(100, 400, 4)\n",
    "    batch_trial = np.arange(100, 400, 4)\n",
    "    DL_pred = []\n",
    "    DL_RMSE = []\n",
    "\n",
    "    for i, j, k in zip(range(4), epochs_trial, batch_trial):\n",
    "        for epoch in range(j):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_tensor)\n",
    "            loss = criterion(outputs, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            DL_predict = model(X_test_tensor).numpy()\n",
    "            DL_RMSE.append(\n",
    "                np.sqrt(mse(Y_test.values / 100, DL_predict.flatten() / 100))\n",
    "            )\n",
    "            DL_pred.append(DL_predict)\n",
    "            print(\"DL_RMSE_{}:{:.6f}\".format(i + 1, DL_RMSE[i]))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def serialize_to_onnx(\n",
    "    model: nn.Module, X_train: pd.DataFrame, save_path=\"uniswap_lp_nn_model\"\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    sample_input = torch.randn(\n",
    "        1, X_train.shape[1]\n",
    "    )\n",
    "\n",
    "    onnx_file_path = save_path + \".onnx\"\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        sample_input,\n",
    "        onnx_file_path,\n",
    "        export_params=True,\n",
    "        opset_version=10,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\"},\n",
    "            \"output\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "    print(f\"Saved serialized ONNX model to {onnx_file_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL_RMSE_1:0.566944\n",
      "DL_RMSE_2:0.568271\n",
      "DL_RMSE_3:0.680286\n",
      "DL_RMSE_4:0.646960\n",
      "Saved serialized ONNX model to uniswap_lp_nn_model.onnx.\n"
     ]
    }
   ],
   "source": [
    "model = train_model(X_train, X_test, Y_train, Y_test)\n",
    "serialize_to_onnx(model, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpile and create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:49:34\u001b[0m.\u001b[1;36m911\u001b[0m\u001b[1m]\u001b[0m No model id provided, checking if model exists âœ…\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:49:34\u001b[0m.\u001b[1;36m912\u001b[0m\u001b[1m]\u001b[0m Model name is: uniswap_lp_nn_model\n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:49:35\u001b[0m.\u001b[1;36m043\u001b[0m\u001b[1m]\u001b[0m Model already exists, using existing model âœ… \n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:49:35\u001b[0m.\u001b[1;36m044\u001b[0m\u001b[1m]\u001b[0m Model found with id -> \u001b[1;36m857\u001b[0m! âœ…\n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:49:35\u001b[0m.\u001b[1;36m490\u001b[0m\u001b[1m]\u001b[0m Version Created with id -> \u001b[1;36m10\u001b[0m! âœ…\n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:49:35\u001b[0m.\u001b[1;36m491\u001b[0m\u001b[1m]\u001b[0m Sending model for transpilation âœ… \n",
      "\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:50:26\u001b[0m.\u001b[1;36m506\u001b[0m\u001b[1m]\u001b[0m Transpilation is fully compatible. Version compiled and Sierra is saved at Giza âœ…\n",
      "\u001b[2K\u001b[32mâ ¼\u001b[0m Transpiling Model...\n",
      "\u001b[1A\u001b[2K\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:50:27\u001b[0m.\u001b[1;36m284\u001b[0m\u001b[1m]\u001b[0m Downloading model âœ…\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m07\u001b[0m-\u001b[1;36m01\u001b[0m \u001b[1;92m10:50:27\u001b[0m.\u001b[1;36m295\u001b[0m\u001b[1m]\u001b[0m model saved at: uniswap_lp_nn\n",
      "\u001b[3m                   Model                   \u001b[0m\n",
      "â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1mid \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mname               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mdescription\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
      "â”‚ 857 â”‚ uniswap_lp_nn_model â”‚             â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\u001b[3m                                    Version                                     \u001b[0m\n",
      "â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1mversion\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1msize \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mstatus \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mmessage\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mdescriâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mcreateâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mlast_uâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mframewoâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”©\n",
      "â”‚ 10      â”‚ 35494 â”‚ COMPLEâ€¦ â”‚ Transpâ€¦ â”‚ Intial  â”‚ 2024-0â€¦ â”‚ 2024-0â€¦ â”‚ CAIRO    â”‚\n",
      "â”‚         â”‚       â”‚         â”‚ Succesâ€¦ â”‚ version â”‚ 07:49:â€¦ â”‚ 07:50:â€¦ â”‚          â”‚\n",
      "â”‚         â”‚       â”‚         â”‚ and     â”‚         â”‚         â”‚         â”‚          â”‚\n",
      "â”‚         â”‚       â”‚         â”‚ Compilâ€¦ â”‚         â”‚         â”‚         â”‚          â”‚\n",
      "â”‚         â”‚       â”‚         â”‚ Succesâ€¦ â”‚         â”‚         â”‚         â”‚          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "Time taken for the operation: 53.046738147735596 seconds\n"
     ]
    }
   ],
   "source": [
    "# transpile the model into Orion Cairo\n",
    "\n",
    "start_time = time.time()\n",
    "! giza transpile uniswap_lp_nn_model.onnx --output-path uniswap_lp_nn\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken for the operation: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2Kâ–°â–°â–°â–°â–±â–±â–± Creating endpoint!t!\n",
      "\u001b[?25h\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m28\u001b[0m \u001b[1;92m18:04:28\u001b[0m.\u001b[1;36m465\u001b[0m\u001b[1m]\u001b[0m Endpoint is successful âœ…\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m28\u001b[0m \u001b[1;92m18:04:28\u001b[0m.\u001b[1;36m467\u001b[0m\u001b[1m]\u001b[0m Endpoint created with id -> \u001b[1;36m411\u001b[0m âœ…\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m28\u001b[0m \u001b[1;92m18:04:28\u001b[0m.\u001b[1;36m467\u001b[0m\u001b[1m]\u001b[0m Endpoint created with endpoint URL: \u001b[4;94mhttps://endpoint-selimsheker-857-3-3f40ae83-7i3yxzspbq-ew.a.run.app\u001b[0m ğŸ‰\n"
     ]
    }
   ],
   "source": [
    "# create giza endpoint\n",
    "! giza endpoints deploy --model-id 857 --version-id 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 857\n",
    "VERSION_ID = 3\n",
    "ENDPOINT_ID = 411"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run verifiable inference using giza sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from giza.agents.model import GizaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(input, model_id, version_id):\n",
    "    model = GizaModel(id=model_id, version=version_id)\n",
    "\n",
    "    (result, proof_id) = model.predict(\n",
    "        input_feed={\"input\": input}, verifiable=True, model_category=\"ONNX_ORION\", dry_run=False\n",
    "    )\n",
    "\n",
    "    return result, proof_id\n",
    "\n",
    "\n",
    "def execution():\n",
    "    input = X_test.iloc[0].to_numpy().reshape(1,2)\n",
    "\n",
    "    (result, proof_id) = prediction(input, MODEL_ID, VERSION_ID)\n",
    "\n",
    "    print(f\"Predicted value for input {input.flatten()} is {result}\")\n",
    "\n",
    "    return result, proof_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting deserialization process...\n",
      "âœ… Deserialization completed! ğŸ‰\n",
      "Predicted value for input [3.22083252e-01 1.66206885e-04] is [[11.82849121]]\n",
      "Proof ID: fd230f906fce43ab9ffcc8096e944da7\n"
     ]
    }
   ],
   "source": [
    "result, proof_id = execution()\n",
    "print(f\"Proof ID: {proof_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROOF_ID = \"fd230f906fce43ab9ffcc8096e944da7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m30\u001b[0m \u001b[1;92m18:49:20\u001b[0m.\u001b[1;36m368\u001b[0m\u001b[1m]\u001b[0m Getting proof from endpoint \u001b[1;36m411\u001b[0m âœ… \n",
      "â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1mid  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mjob_id\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mproving_tâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mcairo_exeâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mmetrics  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mcreated_dâ€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mrequest_â€¦\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â”¡â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”©\n",
      "â”‚ 1273 â”‚ 1465   â”‚            â”‚            â”‚ {'provinâ€¦ â”‚ 2024-06-30 â”‚           â”‚\n",
      "â”‚      â”‚        â”‚            â”‚            â”‚ 34.41734â€¦ â”‚ 15:47:34.â€¦ â”‚           â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    }
   ],
   "source": [
    "# get proof\n",
    "! giza endpoints get-proof --endpoint-id 411 --proof-id \"{PROOF_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m30\u001b[0m \u001b[1;92m18:49:34\u001b[0m.\u001b[1;36m940\u001b[0m\u001b[1m]\u001b[0m Getting proof from endpoint \u001b[1;36m411\u001b[0m âœ… \n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m30\u001b[0m \u001b[1;92m18:49:35\u001b[0m.\u001b[1;36m984\u001b[0m\u001b[1m]\u001b[0m Proof downloaded to uniswap_lp_nn_proof.proof âœ… \n"
     ]
    }
   ],
   "source": [
    "# download proof\n",
    "! giza endpoints download-proof --endpoint-id {ENDPOINT_ID} --proof-id \"{PROOF_ID}\" --output-path uniswap_lp_nn_proof.proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m30\u001b[0m \u001b[1;92m18:49:44\u001b[0m.\u001b[1;36m509\u001b[0m\u001b[1m]\u001b[0m Verifying proof\u001b[33m...\u001b[0m\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m30\u001b[0m \u001b[1;92m18:49:47\u001b[0m.\u001b[1;36m002\u001b[0m\u001b[1m]\u001b[0m Verification result: \u001b[3;92mTrue\u001b[0m\n",
      "\u001b[1;33m[\u001b[0m\u001b[33mgiza\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m2024\u001b[0m-\u001b[1;36m06\u001b[0m-\u001b[1;36m30\u001b[0m \u001b[1;92m18:49:47\u001b[0m.\u001b[1;36m002\u001b[0m\u001b[1m]\u001b[0m Verification time: \u001b[1;36m0.971497554\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# verify\n",
    "!giza verify --proof-id 1273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Giza agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! giza agents create --endpoint-id 395 --name uniswap_lp_agent --description uniswap_lp_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "from logging import getLogger\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from giza.agents import AgentResult, GizaAgent\n",
    "\n",
    "from giza_helpers.addresses import ADDRESSES\n",
    "from giza_helpers.lp_tools import get_tick_range\n",
    "from giza_helpers.uni_helpers import (approve_token, check_allowance, close_position,\n",
    "                         get_all_user_positions, get_mint_params)\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ[\"GIZA-AGENT-TEST-1_PASSPHRASE\"] = os.environ.get(\"GIZA-AGENT-TEST-1_PASSPHRASE\")\n",
    "sepolia_rpc_url = os.environ.get(\"SEPOLIA_RPC_URL\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(realized_vol: float, dec_price_change: float):\n",
    "    pct_change_sq = (100 * dec_price_change) ** 2\n",
    "    X = np.array([[realized_vol, pct_change_sq]])\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    # TODO: implement fetching onchain or from some other source\n",
    "    # hardcoding the values for now\n",
    "    realized_vol = 4.20\n",
    "    dec_price_change = 0.1\n",
    "    return realized_vol, dec_price_change\n",
    "\n",
    "\n",
    "def create_agent(\n",
    "    model_id: int, version_id: int, chain: str, contracts: dict, account: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a Giza agent for the volatility prediction model\n",
    "    \"\"\"\n",
    "    agent = GizaAgent(\n",
    "        contracts=contracts,\n",
    "        id=model_id,\n",
    "        version_id=version_id,\n",
    "        chain=chain,\n",
    "        account=account,\n",
    "    )\n",
    "    return agent\n",
    "\n",
    "\n",
    "def predict(agent: GizaAgent, X: np.ndarray):\n",
    "    \"\"\"\n",
    "    Predict the next day volatility.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Input to the model.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted value.\n",
    "    \"\"\"\n",
    "    prediction = agent.predict(input_feed={\"val\": X}, verifiable=False, job_size=\"XL\")\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def get_pred_val(prediction: AgentResult):\n",
    "    \"\"\"\n",
    "    Get the value from the prediction.\n",
    "\n",
    "    Args:\n",
    "        prediction (dict): Prediction from the model.\n",
    "\n",
    "    Returns:\n",
    "        int: Predicted value.\n",
    "    \"\"\"\n",
    "    # This will block the executon until the prediction has generated the proof\n",
    "    # and the proof has been verified\n",
    "    return prediction.value[0][0]\n",
    "\n",
    "\n",
    "def rebalance_lp(\n",
    "    tokenA_amount: int,\n",
    "    tokenB_amount: int,\n",
    "    pred_model_id: int,\n",
    "    pred_version_id: int,\n",
    "    account=\"dev\",\n",
    "    chain=f\"ethereum:sepolia:{sepolia_rpc_url}\",\n",
    "    nft_id=None,\n",
    "):\n",
    "    logger = getLogger(\"agent_logger\")\n",
    "    nft_manager_address = ADDRESSES[\"NonfungiblePositionManager\"][11155111]\n",
    "    tokenA_address = ADDRESSES[\"UNI\"][11155111]\n",
    "    tokenB_address = ADDRESSES[\"WETH\"][11155111]\n",
    "    pool_address = \"0x287B0e934ed0439E2a7b1d5F0FC25eA2c24b64f7\"\n",
    "    user_address = \"0xCBB090699E0664f0F6A4EFbC616f402233718152\"\n",
    "    pool_fee = 3000\n",
    "    logger.info(\"Fetching input data\")\n",
    "    realized_vol, dec_price_change = get_data()\n",
    "    logger.info(f\"Input data: {realized_vol}, {dec_price_change}\")\n",
    "    X = process_data(realized_vol, dec_price_change)\n",
    "    contracts = {\n",
    "        \"nft_manager\": nft_manager_address,\n",
    "        \"tokenA\": tokenA_address,\n",
    "        \"tokenB\": tokenB_address,\n",
    "        \"pool\": pool_address,\n",
    "    }\n",
    "    agent = create_agent(\n",
    "        model_id=pred_model_id,\n",
    "        version_id=pred_version_id,\n",
    "        chain=chain,\n",
    "        contracts=contracts,\n",
    "        account=account,\n",
    "    )\n",
    "    result = predict(agent, X)\n",
    "    predicted_value = get_pred_val(result)\n",
    "    logger.info(f\"Result: {result}\")\n",
    "    with agent.execute() as contracts:\n",
    "        logger.info(\"Executing contract\")\n",
    "        if nft_id is None:\n",
    "            positions = [\n",
    "                max(get_all_user_positions(contracts.nft_manager, user_address))\n",
    "            ]\n",
    "        else:\n",
    "            positions = [nft_id]\n",
    "        logger.info(f\"Closing the following positions {positions}\")\n",
    "        for nft_id in positions:\n",
    "            close_position(user_address, contracts.nft_manager, nft_id)\n",
    "        logger.info(\"Calculating mint params...\")\n",
    "        _, curr_tick, _, _, _, _, _ = contracts.pool.slot0()\n",
    "        if not check_allowance(\n",
    "            contracts.tokenA, nft_manager_address, account, tokenA_amount\n",
    "        ):\n",
    "            approve_token(contracts.tokenA, nft_manager_address, tokenA_amount)\n",
    "        if not check_allowance(\n",
    "            contracts.tokenB, nft_manager_address, account, tokenB_amount\n",
    "        ):\n",
    "            approve_token(contracts.tokenB, nft_manager_address, tokenB_amount)\n",
    "        tokenA_decimals = contracts.tokenA.decimals()\n",
    "        tokenB_decimals = contracts.tokenB.decimals()\n",
    "        predicted_value = predicted_value / 100 * 1.96  # convert to decimal %\n",
    "        lower_tick, upper_tick = get_tick_range(\n",
    "            curr_tick, predicted_value, tokenA_decimals, tokenB_decimals, pool_fee\n",
    "        )\n",
    "        mint_params = get_mint_params(\n",
    "            user_address,\n",
    "            contracts.tokenA.address,\n",
    "            contracts.tokenB.address,\n",
    "            tokenA_amount,\n",
    "            tokenB_amount,\n",
    "            pool_fee,\n",
    "            lower_tick,\n",
    "            upper_tick,\n",
    "        )\n",
    "        # step 5: mint new position\n",
    "        logger.info(\"Minting new position...\")\n",
    "        contract_result = contracts.nft_manager.mint(mint_params)\n",
    "        logger.info(\"SUCCESSFULLY MINTED A POSITION\")\n",
    "        logger.info(\"Contract executed\")\n",
    "\n",
    "    logger.info(f\"Contract result: {contract_result}\")\n",
    "    pprint.pprint(contract_result.__dict__)\n",
    "    logger.info(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_ID = 858\n",
    "VERSION_ID = 1\n",
    "tokenA_amount = 500\n",
    "tokenB_amount = 500\n",
    "\n",
    "rebalance_lp(tokenA_amount, tokenB_amount, MODEL_ID, VERSION_ID, account=\"giza-agent-test-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## neural network \n",
    "# - hidden layer with shape [32, 16]\n",
    "#     - transpile from onnx: ~1sec\n",
    "#     - inference: ~1sec\n",
    "#     - generate proof: 37.38549 secs\n",
    "#     - verify proof: 0.974234442 secs\n",
    "\n",
    "# - hidden layer with shape [64, 32]\n",
    "#     - transpile from onnx: 42.91086792945862 secs\n",
    "#     - inference: 4.4827117919921875 secs\n",
    "#     - generate proof: 78.88475 secs\n",
    "#     - verify proof: 2.013659523 secs\n",
    "\n",
    "# - hidden layer with shape [128, 64]\n",
    "#     - transpile from onnx: 53.90036916732788 secs\n",
    "#     - inference: 7.7867348194122314 secs\n",
    "#     - generate proof: out of memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
